#!/usr/bin/env python3
# coding=utf8
# Copyright  2022  Jiayu DU

import sys, argparse, json, logging
logging.basicConfig(stream=sys.stderr, level=logging.INFO, format='[%(levelname)s] %(message)s')

import kenlm  # pip install https://github.com/kpu/kenlm/archive/master.zip
import sentencepiece as spm  # pip install sentencepiece


class LanguageModel:
    def __init__(self, path):
        assert(path)
        self.model = kenlm.LanguageModel(path)

    def BosState(self):
        assert(self.model)

        bos = kenlm.State()
        self.model.BeginSentenceWrite(bos)
        return bos

    def LmScore(self, istate, token):
        assert(self.model)

        ostate = kenlm.State()
        score = self.model.BaseScore(istate, token, ostate)
        return score, ostate


# For Input-tape I and Output-tape O, and a search lattice S of size (X * Y), where:
#    X, Y are state-indexes, tokens on tapes are arcs
#
# corresponding indexing system:
#   x-axis step:   { S[x][y] } token:I[x] { S[x+1][y] }
#   y-axis step:   { S[x][y] } token:O[y] { S[x][y+1] }
class LatticeState:
    def __init__(self, x, y):
        self.distance = float('inf')

        self.x, self.y = x, y
        self.lm_state = None

        self.bp = None  # backpointer

class BackPointer:
    def __init__(self, x, y, i, o, edit_type, edit_length = 0.0):
        self.x, self.y = x, y
        self.i, self.o = i, o
        self.edit = edit_type + (F':{edit_length:.1f}' if edit_length else '')


def LevED(
    I,
    O,
    SUB = lambda T1, T2: 0.0 if (T1 == T2) else 1.0,
    INS = lambda T: 1.0,
    DEL = lambda T: 1.0,
):
    X = len(I) + 1
    Y = len(O) + 1
    S = [ [ LatticeState(x, y) for y in range(Y) ] for x in range(X) ]

    # Fill lattice states
    S[0][0].distance = 0.0
    for x in range(X):
        for y in range(Y):
            s = S[x][y]

            if x-1 >= 0 and y-1 >= 0:
                d = S[x-1][y-1].distance + SUB(I[x-1], O[y-1])
                if d < s.distance:
                    s.distance = d
                    s.bp = BackPointer(x-1, y-1, I[x-1], O[y-1], 'C' if I[x-1] == O[y-1] else 'S')

            if x-1 >= 0:
                d = S[x-1][y].distance + DEL(I[x-1])
                if d < s.distance:
                    s.distance = d
                    s.bp = BackPointer(x-1, y, I[x-1], '', 'D')

            if y-1 >= 0:
                d = S[x][y-1].distance + INS(O[y-1])
                if d < s.distance:
                    s.distance = d
                    s.bp = BackPointer(x, y-1, '', O[y-1], 'I')

    # Traceback
    # Loop Invariant: traceback head at state s
    # Termination: s reaches origin
    alignment = []
    s = S[-1][-1]
    while s.bp:
        alignment.append(s.bp)
        s = S[s.bp.x][s.bp.y]
    alignment.reverse()
    return (S[-1][-1].distance, alignment)


def InfoED(I, O, lm):
    SUB = lambda T_i, T_o: 0.0 if T_i == T_o else float('inf')
    DEL = lambda T: 0.0

    X = len(I) + 1
    Y = len(O) + 1
    S = [ [ LatticeState(x, y) for y in range(Y) ] for x in range(X) ]

    # Init origin
    s = S[0][0]
    s.distance, s.lm_state = 0.0, lm.BosState()
    s.bp = None

    # Fill Y axis and cache the information length (Kolmolgorov complexity) of tokens on O-tape
    INS_cache = [0.0]  * len(O)
    for y in range(1, Y):
        # INS from S(0, y-1)  -- O[y-1] -->  S(0, y)
        prev_s, s = S[0][y-1], S[0][y]

        lm_score, lm_state = lm.LmScore(prev_s.lm_state, O[y-1])

        # KenLM returns log_10(prob), we need -log_2(prob) in theory.
        # But since these lengths are used in ratio in the end, 
        # there is no need to convert, except the sign (score -> distance).
        l = -lm_score  
        INS_cache[y-1] = l

        s.distance, s.lm_state = prev_s.distance + l, lm_state
        s.bp = BackPointer(0, y-1, '', O[y-1], 'I', l)


    # Fill the rest lattice states
    for x in range(1, X):
        for y in range(Y):
            s = S[x][y]

            if x-1 >= 0 and y-1 >= 0:
                # SUB: S(x-1, y-1)  -- I[x-1], O[y-1] -->  S(x, y)
                l = SUB(I[x-1], O[y-1])
                d = S[x-1][y-1].distance + l
                if d < s.distance:
                    s.distance, s.lm_state = d, S[0][y].lm_state
                    s.bp = BackPointer(x-1, y-1, I[x-1], O[y-1], 'C' if I[x-1] == O[y-1] else 'S', l)

            if x-1 >= 0:
                # DEL: S(x-1, y)  -- I[x-1] -->  S(x, y)
                l = DEL(I[x-1])
                d = S[x-1][y].distance + l
                if d < s.distance:
                    s.distance, s.lm_state = d, S[x-1][y].lm_state
                    s.bp = BackPointer(x-1, y, I[x-1], '', 'D', l)

            if y-1 >= 0:
                # INS: S(x, y-1)  -- O[y-1] -->  S(x, y)
                l = INS_cache[y-1]
                d = S[x][y-1].distance + l
                if d < s.distance:
                    s.distance, s.lm_state = d, S[0][y].lm_state
                    s.bp = BackPointer(x, y-1, '', O[y-1], 'I', l)

    # Traceback
    # Loop Invariant: traceback head at state s
    # Termination: s reaches origin
    alignment = []
    s = S[-1][-1]
    while s.bp:
        alignment.append(s.bp)
        s = S[s.bp.x][s.bp.y]
    alignment.reverse()

    return (S[-1][-1].distance, S[0][-1].distance, alignment)  # K_I2O, K_O, the shortest edit program


def PrettyAlignment(alignment):
    def token_repr(token):
        return token if token else '*'

    def display_width(token: str):
        def char_width(c):
            return 2 if (c >= '\u4e00') and (c <= '\u9fa5') else 1
        return sum([ char_width(c) for c in token ])

    I = '  I : '
    O = '  O : '
    E = '  E : '
    for arc in alignment:
        i, o = token_repr(arc.i), token_repr(arc.o)
        e = '' if arc.edit == 'C' else arc.edit  # don't bother printing 'C'

        ni, no, ne = display_width(i), display_width(o), display_width(e)
        n = max(ni, no, ne) + 1

        I += i + ' ' * (n-ni)
        O += o + ' ' * (n-no)
        E += e + ' ' * (n-ne)
    return I, O, E

 
def CountEdits(alignment):
    counts = { 'C': 0, 'S': 0, 'I': 0, 'D': 0 }
    for arc in alignment:
        counts[arc.edit] += 1
    return (counts['C'], counts['S'], counts['I'], counts['D'])


def ComputeTER(c, s, i, d):
    assert((c + s + d) != 0)
    num_edits = (s + d + i)
    ref_len = (c + s + d)
    hyp_len = (c + s + i)
    return 100.0 * num_edits / ref_len, 100.0 * num_edits / max(ref_len, hyp_len)


def ComputeNID(K_r2h, K_h2r, K_ref, K_hyp):
    return 100.0 * max(K_r2h, K_h2r) / max(K_ref, K_hyp)


def ComputeSER(num_err_utts, num_utts):
    assert(num_utts != 0)
    return 100.0 * num_err_utts / num_utts


class EvalStats:
    def __init__(self):
        self.refs = 0
        self.hyps = 0
        self.hyp_without_ref = 0
        self.hyp_with_empty_ref = 0

        # SER stats
        self.utts = 0 # seen in both ref & hyp
        self.utts_with_error = 0
        self.sentence_error_rate = 0.0

        # TER stats
        self.C, self.S, self.I, self.D = 0, 0, 0, 0
        self.token_error_rate, self.modified_token_error_rate = 0.0, 0.0

        # NID stats
        self.K_r2h, self.K_h2r, self.K_ref, self.K_hyp = 0.0, 0.0, 0.0, 0.0
        self.normalized_information_distance = 0.0

    def to_kaldi(self):
        return (
            F'%WER {self.token_error_rate:.2f} [ {self.S + self.D + self.I} / {self.C + self.S + self.D}, {self.I} ins, {self.D} del, {self.S} sub ]\n'
            F'%SER {self.sentence_error_rate:.2f} [ {self.utts_with_error} / {self.utts} ]\n'
        )

    def to_summary(self):
        summary = (
            '=============== Overall Evaluation Statistics ===============\n'
            F'EvalStats: {json.dumps(self.__dict__)}\n'
            '--------------------------------------------\n'
            F'refs: {self.refs}\n'
            F'hyps: {self.hyps}\n'
            F'hyp_without_ref: {self.hyp_without_ref}\n'
            F'hyp_with_empty_ref: {self.hyp_with_empty_ref}\n'
            '-------------------- SER -------------------\n'
            F'utts: {self.utts}\n'
            F'utts_with_error: {self.utts_with_error}\n'
            F'SER:  {self.sentence_error_rate:.2f}%\n'
            '-------------------- TER -------------------\n'
            F'ref tokens: {self.C + self.S + self.D:>7}\n'
            F'hyp tokens: {self.C + self.S + self.I:>7}\n'
            F'edits:  {self.S + self.I + self.D:>7}\n'
            F'- COR:  {self.C:>7}\n'
            F'- SUB:  {self.S:>7}\n'
            F'- INS:  {self.I:>7}\n'
            F'- DEL:  {self.D:>7}\n'
            F'TER:  {self.token_error_rate:.2f}%\n'
            F'mTER: {self.modified_token_error_rate:.2f}%\n'
            '-------------------- NID -------------------\n'
            F'K_r2h: {self.K_r2h:.1f} bits\n'
            F'K_h2r: {self.K_h2r:.1f} bits\n'
            F'K_ref: {self.K_ref:.1f} bits\n'
            F'K_hyp: {self.K_hyp:.1f} bits\n'
            F'NID: {self.normalized_information_distance:.2f}%\n'
            '========================================================\n'
        )
        return summary


def LoadText(filepath, format = 'KaldiArk'):
    assert(format == 'KaldiArk') # line format: "key text"
    utts = {}
    with open(filepath, 'r', encoding='utf8') as f:
        for line in f:
            cols = line.strip().split(maxsplit=1)
            if len(cols) == 1 or len(cols) == 2:
                key  = cols[0].strip()
                text = cols[1].strip() if len(cols) == 2 else ''
                if key not in utts:
                    utts[key] = text
                else:
                    raise RuntimeError(F'Found duplicated utterence, key={key}')
    return utts

def GenerateTokenizer(tokenizer):
    if tokenizer == 'word':
        return lambda text: text.strip().split()
    elif tokenizer == 'char':
        return lambda text: [ c for c in text.strip().replace(' ', '') ]
    else:
        model = spm.SentencePieceProcessor()
        model.load(tokenizer)
        return lambda text: model.EncodeAsPieces(text.strip())


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--logk', type=int, default=500 , help='logging interval')
    parser.add_argument('-t', '--tokenizer', type=str, default='word', help='word/char/sentencepiece_model_path')
    parser.add_argument('-m', '--metrics', nargs='+', choices=['TER', 'NID'], default=['TER'], help='nargs: TER, NID, ...')
    parser.add_argument('--lm', type=str, help='Language prior used in NID')
    parser.add_argument('--ref', type=str, required=True, help='REF text')
    parser.add_argument('--hyp', type=str, required=True, help='HYP text')
    parser.add_argument('details', type=str)
    args = parser.parse_args()
    logging.info(args)

    stats = EvalStats()

    logging.info('Loading REF and HYP ...')
    refs = LoadText(args.ref)
    hyps = LoadText(args.hyp)

    # check valid utterances in hyp that have matched non-empty reference
    utts = []
    for utt in sorted(hyps.keys()):
        if utt in refs: # TODO: efficiency
            if refs[utt]: # non-empty reference
                utts.append(utt)
            else:
                stats.hyp_with_empty_ref += 1
                logging.warning(F'Found {utt} with empty reference, skipping...')
        else:
            stats.hyp_without_ref += 1
            logging.warning(F'Found {utt} without reference, skipping...')

    stats.hyps = len(hyps)
    stats.refs = len(refs)

    logging.info('Generating tokenizer ...')
    tokenizer = GenerateTokenizer(args.tokenizer)
    assert(tokenizer)

    if args.lm:
        logging.info('Loading language model ...')
        lm = LanguageModel(args.lm)

    logging.info('Evaluating utterance-level results ...')
    with open(args.details, 'w+', encoding='utf8') as fo:
        for utt in utts:
            if 'TER' in args.metrics:
                distance, alignment = LevED(tokenizer(refs[utt]), tokenizer(hyps[utt]))
                c, s, i, d = CountEdits(alignment)

                stats.C += c
                stats.S += s
                stats.I += i
                stats.D += d

                TER, mTER = ComputeTER(c, s, i, d)

                print(F'{{ "utt":{utt}, "TER":{TER:.2f}, "mTER":{mTER:.2f}, "LevED":{distance}, "cor":{c}, "sub":{s}, "ins":{i}, "del":{d} }}', file=fo)
                print(*PrettyAlignment(alignment), sep='\n', file=fo)
                print('-'* 35, file=fo)

            if 'NID' in args.metrics:
                assert(lm)

                K_r2h, K_hyp, alignment1 = InfoED(tokenizer(refs[utt]), tokenizer(hyps[utt]), lm)
                K_h2r, K_ref, alignment2 = InfoED(tokenizer(hyps[utt]), tokenizer(refs[utt]), lm)

                stats.K_r2h += K_r2h
                stats.K_h2r += K_h2r
                stats.K_ref += K_ref
                stats.K_hyp += K_hyp

                NID = ComputeNID(K_r2h, K_h2r, K_ref, K_hyp)

                # following two alignments are for debugging purpose
                _, _, alignment3 = InfoED([], tokenizer(refs[utt]), lm)
                _, _, alignment4 = InfoED([], tokenizer(hyps[utt]), lm)

                print(F'{{ "utt":{utt}, "NID":{NID:.2f}, "K(hyp|ref)":{K_r2h:.1f}, "K(ref|hyp)":{K_h2r:.1f}, "K(ref)":{K_ref:.1f}, "K(hyp):":{K_hyp:.1f} }}', file=fo)
                print(
                    *PrettyAlignment(alignment1), '',
                    *PrettyAlignment(alignment2), '',
                    *PrettyAlignment(alignment3), '',
                    *PrettyAlignment(alignment4),
                    sep='\n', file=fo,
                )

            print('=' * 70, file=fo)

            if locals().get('TER') or locals().get('NID'):
                stats.utts_with_error += 1
            stats.utts += 1

            if stats.utts % args.logk == 0:
                logging.info(f'{stats.utts:7d} utts evaluated.')
        logging.info('Evaluating corpus-level results ...')
        stats.sentence_error_rate = ComputeSER(stats.utts_with_error, stats.utts)

        if 'TER' in args.metrics:
            stats.token_error_rate, stats.modified_token_error_rate = ComputeTER(stats.C, stats.S, stats.I, stats.D)
        if 'NID' in args.metrics:
            stats.normalized_information_distance = ComputeNID(stats.K_r2h, stats.K_h2r, stats.K_ref, stats.K_hyp)

        print(stats.to_summary(), file=fo)
    logging.info(f'Total {stats.utts} utts evaluated.')

    #print(stats.to_kaldi())
    print(stats.to_summary())

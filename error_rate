#!/usr/bin/env python3
# coding=utf8

# Copyright  2022  Jiayu DU

import sys
import argparse
import json

import logging
logging.basicConfig(stream=sys.stderr, level=logging.INFO, format='[%(levelname)s] %(message)s')

DEBUG = False

def GetEditType(i, o):
    if not i and o:
        return 'I'
    elif i and not o:
        return 'D'
    elif i == o:
        return 'C'
    elif i != o:
        return 'S'
    else:
        raise RuntimeError


class LatticeState:
    def __init__(self, x, y):
        self.score = -float('inf')
        self.x, self.y = x, y
        self._x, self._y = None, None # backpointer

class AlignmentArc:
    def __init__(self, src, dst, i, o):
        self.src = src
        self.dst = dst
        self.i = i
        self.o = o
        self.edit = GetEditType(i, o)


def EditDistance(
    I,
    O,
    SUB = lambda T1, T2: 0.0 if (T1 == T2) else -1.0,
    INS = lambda T: -1.0,
    DEL = lambda T: -1.0,
):
    assert(len(I) != 0)
    def print_lattice(S, X, Y, fstream):
        print(file=fstream)
        for y in range(Y):
            for x in range(X):
                print(F'[{x},{y}]:{S[x][y].score:4.3f}:({S[x][y]._x},{S[x][y]._y}) ', end='', file=fstream)
            print(file=fstream)

    # For Input I and Output O, a search lattice S of size X*Y
    # coordinating system: 
    #       ...
    #   { S[x][y+1] }
    #        |
    #    token:O[y]
    #        |
    #   { S[x][y] } -- token:I[x] -- { S[x+1][y] } ...
    X = len(I) + 1
    Y = len(O) + 1
    S = [ [ LatticeState(x, y) for y in range(Y) ] for x in range(X) ]

    # Init origin
    s = S[0][0]
    s.score = 0.0
    s._x, s._y = None, None

    # Fill x-axis
    for x in range(1, X):
        s = S[x][0]
        s.score = S[x-1][0].score + DEL(I[x-1])
        s._x, s._y = x-1, 0

    # Fill y-axis
    for y in range(1, Y):
        s = S[0][y]
        s.score = S[0][y-1].score + INS(O[y-1])
        s._x, s._y = 0, y-1

    # Fill lattice internal
    for x in range(1, X):
        for y in range(1, Y):
            s = S[x][y]

            score = S[x-1][y-1].score + SUB(I[x-1], O[y-1])
            if score >= s.score:
                s.score = score
                s._x, s._y = x-1, y-1

            score = S[x-1][y].score + DEL(I[x-1])
            if score >= s.score:
                s.score = score
                s._x, s._y = x-1, y

            score = S[x][y-1].score + INS(O[y-1])
            if score >= s.score:
                s.score = score
                s._x, s._y = x, y-1

    if DEBUG:
        print_lattice(S, X, Y, sys.stderr)

    # Traceback
    # Loop invariant: traceback head at state q
    # Termination: q reaches origin
    alignment = []
    q = S[-1][-1]
    while not (q.x == 0 and q.y == 0):
        p = S[q._x][q._y]
        src, dst = (p.x, p.y), (q.x, q.y)
        if q.x == p.x + 1 and q.y == p.y + 1: # Substitution or correct
            arc = AlignmentArc(src, dst, I[p.x], O[p.y])
        elif q.x == p.x + 1 and q.y == p.y: # Deletion
            arc = AlignmentArc(src, dst, I[p.x], '')
        elif q.x == p.x and q.y == p.y + 1: # Insertion
            arc = AlignmentArc(src, dst, '', O[p.y])
        else:
            raise RuntimeError
        alignment.append(arc)
        q = p
    alignment.reverse()
    return (alignment, S[-1][-1].score)


def PrettyAlignment(alignment):
    def token_repr(token):
        return token if token else '*'

    def edit_repr(edit):
        return '' if edit == 'C' else edit

    def display_width(token: str):
        def char_width(c):
            return 2 if (c >= '\u4e00') and (c <= '\u9fa5') else 1
        return sum([ char_width(c) for c in token ])

    H = '  HYP  : '
    R = '  REF  : '
    E = '  EDIT : '
    for arc in alignment:
        r, h, e = token_repr(arc.i), token_repr(arc.o), edit_repr(arc.edit)
        nr, nh, ne = display_width(r), display_width(h), display_width(e)
        n = max(nr, nh, ne) + 1

        H += h + ' ' * (n-nh)
        R += r + ' ' * (n-nr)
        E += e + ' ' * (n-ne)
    return H, R, E

 
def CountEdits(alignment):
    counts = { 'C': 0, 'S': 0, 'I': 0, 'D': 0 }
    for arc in alignment:
        counts[arc.edit] += 1
    return (counts['C'], counts['S'], counts['I'], counts['D'])


def ComputeTokenErrorRate(c, s, i, d):
    assert((c + s + d) != 0)
    num_edits = (s + d + i)
    ref_len = (c + s + d)
    hyp_len = (c + s + i)
    return 100.0 * num_edits / ref_len, 100.0 * num_edits / max(ref_len, hyp_len)


def ComputeSentenceErrorRate(num_err_utts, num_utts):
    assert(num_utts != 0)
    return 100.0 * num_err_utts / num_utts


class EvalStats:
    def __init__(self):
        self.refs = 0
        self.hyps = 0
        self.hyp_without_ref = 0
        self.hyp_with_empty_ref = 0

        self.utts = 0 # seen in both ref & hyp
        self.utts_with_error = 0
        self.sentence_error_rate = 0.0

        self.C, self.S, self.I, self.D = 0, 0, 0, 0
        self.token_error_rate = 0.0
        self.modified_token_error_rate = 0.0

    def to_kaldi(self):
        return (
            F'%WER {self.token_error_rate:.2f} [ {self.S + self.D + self.I} / {self.C + self.S + self.D}, {self.I} ins, {self.D} del, {self.S} sub ]\n'
            F'%SER {self.sentence_error_rate:.2f} [ {self.utts_with_error} / {self.utts} ]\n'
        )

    def to_summary(self):
        summary = (
            '=============== Overall Evaluation Statistics ===============\n'
            F'EvalStats: {json.dumps(self.__dict__)}\n'
            '--------------------------------------------------------\n'
            F'refs: {self.refs}\n'
            F'hyps: {self.hyps}\n'
            F'hyp_without_ref: {self.hyp_without_ref}\n'
            F'hyp_with_empty_ref: {self.hyp_with_empty_ref}\n'
            F'utts: {self.utts}\n'
            F'tokens: {self.C + self.S + self.D:>7}\n'
            F'edits:  {self.S + self.I + self.D:>7}\n'
            F'- COR:  {self.C:>7}\n'
            F'- SUB:  {self.S:>7}\n'
            F'- INS:  {self.I:>7}\n'
            F'- DEL:  {self.D:>7}\n'
            F'TER:  {self.token_error_rate:.2f}%\n'
            F'mTER: {self.modified_token_error_rate:.2f}%\n'
            F'SER:  {self.sentence_error_rate:.2f}%\n'
            '========================================================\n'
        )
        return summary


def LoadKaldiArc(filepath):
    utts = {}
    with open(filepath, 'r', encoding='utf8') as f:
        for line in f:
            cols = line.strip().split(maxsplit=1)
            if len(cols) == 1 or len(cols) == 2:
                key  = cols[0].strip()
                text = cols[1].strip() if len(cols) == 2 else ''
                if key not in utts:
                    utts[key] = text
                else:
                    raise RuntimeError(F'Found duplicated utterence, key={key}')
    return utts


def GenerateTokenizer(tokenizer_type):
    if tokenizer_type == 'whitespace':
        def word_tokenizer(text):
            return text.strip().split()
        tokenizer = word_tokenizer
    elif tokenizer_type == 'char':
        def char_tokenizer(text):
            return [ c for c in text.strip().replace(' ', '') ]
        tokenizer = char_tokenizer
    else:
        raise RuntimeError
    return tokenizer


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--tokenizer', choices=['whitespace', 'char'], default='whitespace', help='whitespace for WER, char for CER')
    parser.add_argument('--logk', type=int, default=500 , help='whitespace for WER, char for CER')
    parser.add_argument('-x', '--ref', type=str, required=True, help='input reference file')
    parser.add_argument('-y', '--hyp', type=str, required=True, help='input hypothesis file')
    parser.add_argument('details', type=str)
    args = parser.parse_args()
    logging.info(args)

    stats = EvalStats()

    logging.info('Loading REF and HYP ...')
    refs = LoadKaldiArc(args.ref)
    hyps = LoadKaldiArc(args.hyp)

    # check valid utterances in hyp that have matched non-empty reference
    utts = []
    for utt in sorted(hyps.keys()):
        if utt in refs: # TODO: efficiency
            if refs[utt]: # non-empty reference
                utts.append(utt)
            else:
                stats.hyp_with_empty_ref += 1
                logging.warning(F'Found {utt} with empty reference, skipping...')
        else:
            stats.hyp_without_ref += 1
            logging.warning(F'Found {utt} without reference, skipping...')

    stats.hyps = len(hyps)
    stats.refs = len(refs)
    stats.utts = len(utts)

    logging.info('Generating tokenizer ...')
    tokenizer = GenerateTokenizer(args.tokenizer)
    assert(tokenizer)

    logging.info('Evaluating ...')
    ndone = 0
    with open(args.details, 'w+', encoding='utf8') as fo:
        for utt in utts:
            alignment, score = EditDistance(
                tokenizer(refs[utt]),
                tokenizer(hyps[utt]),
            )
            c, s, i, d = CountEdits(alignment)
            ter, mter = ComputeTokenErrorRate(c, s, i, d)
            # utt-level details
            print(F'{{"utt":{utt}, "score":{score}, "TER":{ter:.2f}, "mTER":{mter:.2f}, "cor":{c}, "sub":{s}, "ins":{i}, "del":{d}}}', file=fo)
            print(*PrettyAlignment(alignment), sep='\n', file=fo)

            stats.C += c
            stats.S += s
            stats.I += i
            stats.D += d

            if ter > 0:
                stats.utts_with_error += 1

            ndone += 1
            if ndone % args.logk == 0:
                logging.info(f'{ndone:7d} utts evaluated.')
    logging.info(f'{ndone:7d} utts evaluated in total.')

    # corpus level evaluation result
    stats.token_error_rate, stats.modified_token_error_rate = ComputeTokenErrorRate(stats.C, stats.S, stats.I, stats.D)
    stats.sentence_error_rate = ComputeSentenceErrorRate(stats.utts_with_error, stats.utts)

    print(stats.to_kaldi())
    print(stats.to_summary())
